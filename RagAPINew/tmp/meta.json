{
    "chunks": [
        {
            "text_preview": "Artificial Intelligence\nDr. Indu Joshi\nAssistant Professor at\nIndian Institute of Technology Mandi\n30 June 2025\n\nIntroduction to Artificial Intelligence and Machine Learning\n\u2022 Artificial Intelligence (AI) refers to the simulation of human\nintelligence in machines that are programmed to think and act\nlike humans.\n\u2022 AI is a broad field encompassing systems that perform tasks\nrequiring human intelligence, such as reasoning, learning, and\nproblem-solving.\n\u2022 Machine Learning (ML) is a subset of AI that enables\nmachines to learn from data and improve over time without\nexplicit programming.\n\nWhen Do We Need Machine Learning?\n\u2022 When do we need machine learning rather than directly\nprogram our computers to carry out the task at hand?\n\u2022 Two aspects of a given problem may call for the use of\nprograms that learn and improve on the basis of their\n\u201cexperience\u201d: the problem\u2019s complexity and the need for\nadaptivity.\n\nTypes of Learning",
            "source": "RagAPINew/tmp\\randomID2_common.txt"
        },
        {
            "text_preview": "Types of Learning\n\nTypes of Learning\n\u2022 Supervised Learning: Learning from labeled data.\n\u2022 Unsupervised Learning: Learning patterns from unlabeled\ndata.\n\u2022 Semi-Supervised Learning: Learning from a mix of labeled\nand unlabeled data.\n\u2022 Reinforcement Learning: Learning through rewards and\npenalties.\n\nSupervised Learning\n\u2022 Data has input-output pairs (labeled data).\n\u2022 Goal: Learn a mapping from input to output.\n\u2022 Examples: Classification, regression.\n\u2022 Applications: Spam email detection, predicting house prices.\n\nUnsupervised Learning\n\u2022 No labeled data.\n\u2022 Goal: Discover patterns and structure.\n\u2022 Examples: Clustering, dimensionality reduction.\n\u2022 Applications: Customer segmentation, anomaly detection.\n\nSupervised vs. Unsupervised Learning\n\nSemi-supervised Learning\n\u2022 Combines labeled and unlabeled data.\n\u2022 Goal: Improve learning efficiency using limited labeled data.\n\u2022 Applications: Text classification, Medical diagnosis (limited\nlabeled data).\n\nExample-Semi-supervised",
            "source": "RagAPINew/tmp\\randomID2_common.txt"
        },
        {
            "text_preview": "Example-Semi-supervised\n\nReinforcement Learning\n\u2022 Agent learns by interacting with the environment.\n\u2022 Receives rewards for desired actions and penalties for\nundesired ones.\n\u2022 Goal: Maximize cumulative reward.\n\u2022 Applications: game playing (e.g., AlphaGo), robotics.\n\nKey Components of Reinforcement Learning\n\u2022 Agent: The learner.\n\u2022 Environment: The world the agent interacts with.\n\u2022 State: Current situation of the agent.\n\u2022 Action: What the agent can do.\n\u2022 Reward: Feedback from the environment.\n\nReinforcement Learning\n\nML Pipeline\n\nModel Training\n\u2022 Model training is the process of teaching a machine learning\nalgorithm to make predictions or decisions.\n\u2022 Key components:\n\u2022 Data preparation.\n\u2022 Model selection.\n\u2022 Learning from data.\n\u2022 Goal: Minimize error and generalize well to new data.\n\nCollecting and Preparing Data\n\u2022 Collect data from various sources.\n\u2022 Clean and preprocess data (e.g., handle missing values, scale\nfeatures).\n\u2022 Split into training, validation, and testing datasets.",
            "source": "RagAPINew/tmp\\randomID2_common.txt"
        },
        {
            "text_preview": "Model Selection\n\u2022 Choose an algorithm based on the problem type:\n\u2022 Classification (e.g., neural network, SVM).\n\u2022 Regression (e.g., linear regression).\n\u2022 Clustering (e.g., k-means). Consider model complexity and\ndata size.\n\nTraining\n\u2022 The model learns patterns in the training data.\n\u2022 Key steps:\n\u2022 Initialize model parameters.\n\u2022 Compute predictions.\n\u2022 Calculate error (loss function).\n\u2022 Update parameters using optimization algorithms (e.g.,\ngradient descent).\n\nLoss Function\n\u2022 Quantifies the error between predicted and actual values.\n\u2022 Examples:\n\u2022 Mean Squared Error (MSE) for regression.\n\u2022 Cross-Entropy Loss for classification.\n\u2022 Goal: Minimize the loss function.\n\nValidation\n\u2022 Evaluate the model on the validation dataset.\n\u2022 Monitor metrics like accuracy, precision, recall, or MSE.\n\u2022 Avoid overfitting by tuning hyperparameters.\n\nTesting\n\u2022 Final evaluation on unseen data (testing dataset).\n\u2022 Ensure the model generalizes well.\n\u2022 Use metrics to assess performance.",
            "source": "RagAPINew/tmp\\randomID2_common.txt"
        },
        {
            "text_preview": "Model Evaluation: Training, Validation, and Testing\n\nWhy Do We Split Data?\n\u2022 Avoid overfitting and underfitting.\n\u2022 Assess model performance on unseen data.\n\u2022 Fine-tune hyperparameters.\n\nTraining Set\n\u2022 Used to train the machine learning model.\n\u2022 Model learns patterns and relationships in the data.\n\u2022 Typically the largest portion of the dataset ( 60-70%).\n\nValidation Set\n\u2022 Used for hyperparameter tuning.\n\u2022 Helps prevent overfitting by evaluating the model during\ntraining.\n\u2022 Typically 15-20% of the dataset.\n\nTest Set\n\u2022 Used to evaluate final model performance.\n\u2022 Provides an unbiased estimate of the model\u2019s accuracy.\n\u2022 Typically 15-20% of the dataset.\n\nCross Validation\n\u2022 Ensures robustness by using multiple validation sets.\n\u2022 Common techniques: K-Fold, Leave-One-Out.\n\u2022 Helps when datasets are small.",
            "source": "RagAPINew/tmp\\randomID2_common.txt"
        },
        {
            "text_preview": "Trade-off\n\u2022 In machine learning, there is a trade-off between\n\u2022 Complex models that fit the training data well\n\u2022 simpler models that may generalise better\n\u2022 As the amount of training data increases, the generalization\nerror decreases.\n\nUnderfitting and Overfitting\nAchieving the Right Balance for Optimal Model Performance.\n\nUnderfitting and Overfitting\n\u2022 Underfitting: Model is too simple to capture patterns in the\ndata.\n\u2022 Overfitting: Model is too complex and captures noise instead\nof patterns.\n\nUnderfitting\n\u2022 Occurs when the model cannot capture the underlying trends.\n\u2022 Characteristics:\n\u2022 Poor performance on both training and test sets.\n\u2022 Causes:\n\u2022 Model is too simple.\n\u2022 Insufficient training or features.\n\u2022 Example: Linear model for non-linear data.",
            "source": "RagAPINew/tmp\\randomID2_common.txt"
        },
        {
            "text_preview": "Overfitting\n\u2022 Occurs when the model captures noise along with the signal.\n\u2022 Characteristics:\n\u2022 Excellent performance on training but poor on test data.\n\u2022 Causes:\n\u2022 Model is too complex.\n\u2022 Insufficient or noisy data.\nExample: A very wiggly curve fitting random noise in the data.\n\nPreventing Underfitting\n\u2022 Increase model complexity.\n\u2022 Train longer with sufficient data.\n\u2022 Improve feature engineering.\n\nPreventing Overfitting\n\u2022 Simplify the model.\n\u2022 Use regularization techniques (L1, L2).\n\u2022 Data augmentation and more training data.\n\u2022 Use dropout in neural networks.\n\u2022 Cross-validation.\n\nExperimental Evaluation of Machine Learning Algorithms\n\u2022 It is essential to evaluate the performance of learning systems\nbecause these systems are usually designed to predict the class\nof future unlabelled data points.\n\u2022 Metrics for performance evaluation include: error, accuracy\nand precision/recall.",
            "source": "RagAPINew/tmp\\randomID2_common.txt"
        },
        {
            "text_preview": "Evaluation of Predictions\n\u2022 Suppose we want to make a prediction of a value for a target\nfeature on example x.\n\u2022 y is the observed value of target feature on example x.\n\u2022 \u02c6y is the predicted value of target feature on example x.\n\u2022 How to quantify the error?\n\nEvaluation Matrix-Regression\n\u2022 Absolute Error:\n1\nn\nn\nX\ni=1\n|\u02c6y \u2212y|\n\u2022 Sum of Squares Error:\n1\nn\nn\nX\ni=1\n(\u02c6y \u2212y)2\n\nEvaluation Matrix-Classification\nAccuracy =TP + TN\nP + N\nPrecision =\nTP\nTP + FP\nRecall =TP\nP\n\u2022 When choosing a metric, one must consider the implications\nof false positives and false negatives on that application.\n\u2022 In scenarios like healthcare, a high recall is essential to ensure\nall disease cases are identified despite the risk of false\npositives.\n\u2022 In contrast, precision may be more important in scenarios like\nspam detection, where false positives (legitimate emails\nmarked as spam) can be highly disruptive.\n\nAccuracy\nAccuracy =TP + TN\nP + N\n\u2022 Works well for balanced datasets.\n\u2022 Misleading for imbalanced datasets.",
            "source": "RagAPINew/tmp\\randomID2_common.txt"
        },
        {
            "text_preview": "Precision and Recall\nPrecision =\nTP\nTP + FP\nRecall =TP\nP\n\u2022 Precision signifies the proportion of true positives among\npredicted positives.\n\u2022 Recall signifies the proportion of true positives among actual\npositives.\n\u2022 Trade-off between precision and recall.\n\nF1 Score: Balancing Precision and Recall\nF1 = 2 \u00b7 Precision \u00b7 Recall\nPrecision + Recall\n\u2022 Harmonic mean of precision and recall.\n\u2022 Useful for imbalanced datasets.\n\nROC and AUC\n\u2022 Receiver Operating Characteristic (ROC) Curve: Plots True\nPositive Rate (TPR) vs. False Positive Rate (FPR).\n\u2022 AUC (Area Under the Curve): Measures the ability to\ndistinguish between classes.\n\u2022 Higher AUC indicates better performance.\n\nExample",
            "source": "RagAPINew/tmp\\randomID2_common.txt"
        },
        {
            "text_preview": "Example\n\nExample\nAccuracy =\nTP + TN\nTP + TN + FP + FN\n=\n980 + 2\n980 + 2 + 0 + 18 = 982\n1000 = 0.982 (or 98.2%)\nPrecision (for Disease class) =\nTP\nTP + FP =\n2\n2 + 0 = 2\n2 = 1.0 (or 100%)\nRecall =\nTP\nTP + FN =\n2\n2 + 18 = 2\n20 = 0.1 (or 10%)\nF1 = 2 \u00d7 Precision \u00d7 Recall\nPrecision + Recall\n= 2 \u00d7 1.0 \u00d7 0.1\n1.0 + 0.1 = 2 \u00d7 0.1\n1.1 = 2 \u00d7 0.0909 = 0.1818 (or 18.18%)\n\nExample\n\nExample\n\u2022 Accuracy: 98.2% \u2013 While high, it does not account for our\ndataset\u2019s gravity of false negatives.\n\u2022 Precision: 100% \u2013 The model\u2019s every positive prediction is\ncorrect, but this does not consider the number of missed true\npositives.\n\u2022 Recall: 10% \u2013 This low score indicates that the model fails to\nidentify 90% of actual positive cases, a critical flaw in specific\ncontexts such as disease diagnosis.\n\u2022 F1 Score: Approximately 0.18 \u2013 This score balances precision\nand recall, but in this case, it leans towards precision due to\nthe very low recall.\n\nThank You\nContact: indujoshi@iitmandi.ac.in",
            "source": "RagAPINew/tmp\\randomID2_common.txt"
        }
    ]
}